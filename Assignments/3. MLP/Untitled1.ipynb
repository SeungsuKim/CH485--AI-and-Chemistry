{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.n_bits = 2048\n",
    "args.n_splits = 5\n",
    "args.test_size = 0.15\n",
    "args.val_size = 0.15\n",
    "args.shuffle = True\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.float)\n",
    "\n",
    "tox_types = ['nr-ahr', 'nr-ar-lbd', \n",
    "             'nr-ar', 'nr-aromatase', \n",
    "             'nr-er-lbd', 'nr-er', \n",
    "             'nr-ppar-gamma', 'sr-are', \n",
    "             'sr-atad5', 'sr-hse', \n",
    "             'sr-mmp', 'sr-p53']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <utils.ToxDataset object at 0x1a239200b8>, 'val': <utils.ToxDataset object at 0x1a28d66eb8>, 'test': <utils.ToxDataset object at 0x1a28d665f8>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28d54390>, 'val': <utils.ToxDataset object at 0x10a560748>, 'test': <utils.ToxDataset object at 0x1a28d66630>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28d66710>, 'val': <utils.ToxDataset object at 0x1a28d666d8>, 'test': <utils.ToxDataset object at 0x1a28d66668>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28d666a0>, 'val': <utils.ToxDataset object at 0x1a28e88048>, 'test': <utils.ToxDataset object at 0x1a28e88080>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88198>, 'val': <utils.ToxDataset object at 0x1a28e88208>, 'test': <utils.ToxDataset object at 0x1a28e881d0>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88320>, 'val': <utils.ToxDataset object at 0x1a28e88358>, 'test': <utils.ToxDataset object at 0x1a28e88390>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88518>, 'val': <utils.ToxDataset object at 0x1a28e884a8>, 'test': <utils.ToxDataset object at 0x1a28e884e0>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88630>, 'val': <utils.ToxDataset object at 0x1a28e886a0>, 'test': <utils.ToxDataset object at 0x1a28e88668>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e887f0>, 'val': <utils.ToxDataset object at 0x1a28e88828>, 'test': <utils.ToxDataset object at 0x1a28e887b8>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88940>, 'val': <utils.ToxDataset object at 0x1a28e889b0>, 'test': <utils.ToxDataset object at 0x1a28e88978>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88b00>, 'val': <utils.ToxDataset object at 0x1a28e88b70>, 'test': <utils.ToxDataset object at 0x1a28e88b38>}\n",
      "{'train': <utils.ToxDataset object at 0x1a28e88c50>, 'val': <utils.ToxDataset object at 0x1a28e88cf8>, 'test': <utils.ToxDataset object at 0x1a28e88cc0>}\n"
     ]
    }
   ],
   "source": [
    "dict_partition = dict()\n",
    "for tox_type in tox_types:\n",
    "    args.tox_type = 'tox21/'+tox_type\n",
    "    smiles, toxs = make_dataset(args)\n",
    "    args.tox_type = tox_type\n",
    "    dict_partition[args.tox_type] = partition(smiles, toxs, args)\n",
    "    print(dict_partition[args.tox_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tox21/nr-ahr': {'train': <utils.ToxDataset at 0x1a23794908>,\n",
       "  'val': <utils.ToxDataset at 0x10a4aecf8>,\n",
       "  'test': <utils.ToxDataset at 0x10a554a20>},\n",
       " 'tox21/nr-ar-lbd': {'train': <utils.ToxDataset at 0x1a23889ef0>,\n",
       "  'val': <utils.ToxDataset at 0x1a23889f60>,\n",
       "  'test': <utils.ToxDataset at 0x1a23889eb8>},\n",
       " 'tox21/nr-ar': {'train': <utils.ToxDataset at 0x1a23889f98>,\n",
       "  'val': <utils.ToxDataset at 0x1a23889f28>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28048>},\n",
       " 'tox21/nr-aromatase': {'train': <utils.ToxDataset at 0x1a23a28198>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a281d0>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28208>},\n",
       " 'tox21/nr-er-lbd': {'train': <utils.ToxDataset at 0x1a23a282e8>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a28358>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28320>},\n",
       " 'tox21/nr-er': {'train': <utils.ToxDataset at 0x1a23a28470>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a284a8>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a284e0>},\n",
       " 'tox21/nr-ppar-gamma': {'train': <utils.ToxDataset at 0x1a23a28668>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a285f8>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28630>},\n",
       " 'tox21/sr-are': {'train': <utils.ToxDataset at 0x1a23a28780>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a287f0>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a287b8>},\n",
       " 'tox21/sr-atad5': {'train': <utils.ToxDataset at 0x1a23a28940>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a28978>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28908>},\n",
       " 'tox21/sr-hse': {'train': <utils.ToxDataset at 0x1a23a28a90>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a28b00>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28ac8>},\n",
       " 'tox21/sr-mmp': {'train': <utils.ToxDataset at 0x1a23a28c50>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a28cc0>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28c88>},\n",
       " 'tox21/sr-p53': {'train': <utils.ToxDataset at 0x1a23a28da0>,\n",
       "  'val': <utils.ToxDataset at 0x1a23a28dd8>,\n",
       "  'test': <utils.ToxDataset at 0x1a23a28e10>}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_result = dict()\n",
    "dict_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(args):\n",
    "    layers = list()\n",
    "    layers.append(nn.Linear(args.input_dim, args.hidden_dim))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    for i in range(args.n_layer-2):\n",
    "        layers.append(nn.Linear(args.hidden_dim, args.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        if args.dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(args.dropout_rate))\n",
    "            \n",
    "    layers.append(nn.Linear(args.hidden_dim, args.output_dim))\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, criterion, args):\n",
    "    dict_train_loss = dict()\n",
    "    \n",
    "    data_iter = DataLoader(partition['train'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=args.shuffle)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        train_loss_epoch = 0\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            fps = torch.tensor(batch[0], device=args.device)\n",
    "            toxs = torch.tensor(batch[1], device=args.device)\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred_toxs = model(fps)\n",
    "            pred_toxs.require_grad = False\n",
    "            \n",
    "            train_loss = criterion(pred_toxs, toxs)\n",
    "            train_loss_epoch += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        dict_train_loss[(epoch+1)*args.batch_size/len(data_iter)] = train_loss.item()\n",
    "    \n",
    "    return model, pd.Series(dict_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(model, parition, criterion, args):\n",
    "    dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args):\n",
    "    dict_train_loss = dict()\n",
    "    dict_val_loss = dict()\n",
    "    \n",
    "    model = construct_model(args)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    optimizer = args.optim(model.parameters(), \n",
    "                           lr=args.lr, \n",
    "                           weight_decay=args.l2_coef)\n",
    "    \n",
    "    data_train = DataLoader(args.partition['train'],\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=args.shuffle)\n",
    "    \n",
    "    data_val = DataLoader(args.partition['val'],\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=args.shuffle)\n",
    "    \n",
    "    data_test = DataLoader(args.partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=args.shuffle)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i, batch in enumerate(data_train):\n",
    "            fps = torch.tensor(batch[0], device=args.device)\n",
    "            toxs = torch.tensor(batch[1], device=args.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred_toxs = model(fps)\n",
    "            pred_toxs.require_grad = False\n",
    "            \n",
    "            train_loss = criterion(pred_toxs, toxs)\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        dict_train_loss[epoch] = train_loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(data_val):\n",
    "                fps = torch.tensor(batch[0], device=args.device)\n",
    "                toxs = torch.tensor(batch[1], device=args.device)\n",
    "\n",
    "                pred_toxs = model(fps)\n",
    "                \n",
    "                val_loss = criterion(pred_toxs, toxs)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "                \n",
    "        dict_val_loss[epoch] = val_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    output = list()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_test):\n",
    "            fps = torch.tensor(batch[0], device=args.device)\n",
    "            toxs = torch.tensor(batch[1], device=args.device)\n",
    "            \n",
    "            pred_toxs = model(fps)\n",
    "            \n",
    "            output.append([0 if x<0.5 else 1 for x in pred_toxs])\n",
    "        \n",
    "        output = np.concatenate(output, axis=0)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-994a6ef30cb0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-994a6ef30cb0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def experiment(partition)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "args.exp_name = 'exp_hidden_dim'\n",
    "args.input_dim = 2048\n",
    "args.output_dim = 1\n",
    "args.n_layer = 3\n",
    "args.dropout_rate = 0.0\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0\n",
    "args.optim = optim.Adam\n",
    "args.criterion = nn.BCELoss()\n",
    "args.epoch = 100\n",
    "args.batch_size = 128\n",
    "args.device = 'cpu'\n",
    "\n",
    "exp_result[args.exp_name] = list()\n",
    "hidden_dims = [64, 128, 256, 1024]\n",
    "\n",
    "for tox_type in tox_types[:1]:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        args.tox_type = tox_type\n",
    "        args.hidden_dim = hidden_dim\n",
    "        args.partition = dict_partition[args.tox_type]\n",
    "        experiemnt(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_toxs = [0.1, 0.1, 0.8]\n",
    "output = [0 if x<0.5 else 1 for x in pred_toxs]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nr-ahr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bfae9bbb2085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                        weight_decay=args.l2_coef)\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m data_train = DataLoader(dict_partition[args.tox_type]['train'],\n\u001b[0m\u001b[1;32m     28\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         shuffle=args.shuffle)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nr-ahr'"
     ]
    }
   ],
   "source": [
    "args.exp_name = 'exp_hidden_dim'\n",
    "args.input_dim = 2048\n",
    "args.output_dim = 1\n",
    "args.n_layer = 3\n",
    "args.dropout_rate = 0.0\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0\n",
    "args.optim = optim.Adam\n",
    "args.criterion = nn.BCELoss()\n",
    "args.epoch = 100\n",
    "args.batch_size = 128\n",
    "args.device = 'cpu'\n",
    "\n",
    "args.tox_type = tox_types[0]\n",
    "args.hidden_dim = 128\n",
    "\n",
    "dict_train_loss = dict()\n",
    "dict_val_loss = dict()\n",
    "\n",
    "model = construct_model(args)\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = args.optim(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.l2_coef)\n",
    "\n",
    "data_train = DataLoader(dict_partition[args.tox_type]['train'],\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle)\n",
    "\n",
    "data_val = DataLoader(dict_partition[args.tox_type]['val'],\n",
    "                      batch_size=args.batch_size,\n",
    "                      shuffle=args.shuffle)\n",
    "\n",
    "data_test = DataLoader(dict_partition[args.tox_type]['test'],\n",
    "                       batch_size=args.batch_size,\n",
    "                       shuffle=args.shuffle)\n",
    "\n",
    "print(len(data_train))\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        fps = torch.tensor(batch[0], device=args.device)\n",
    "        toxs = torch.tensor(batch[1], device=args.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_toxs = model(fps)\n",
    "        pred_toxs.require_grad = False\n",
    "\n",
    "        train_loss = criterion(pred_toxs, toxs)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \"\\t batch: \", i, \"\\t Training\")\n",
    "\n",
    "    dict_train_loss[epoch] = epoch_train_loss\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_val):\n",
    "            fps = torch.tensor(batch[0], device=args.device)\n",
    "            toxs = torch.tensor(batch[1], device=args.device)\n",
    "\n",
    "            pred_toxs = model(fps)\n",
    "\n",
    "            val_loss = criterion(pred_toxs, toxs)\n",
    "            epoch_val_loss += val_loss.item()\n",
    "\n",
    "    dict_val_loss[epoch] = epoch_val_loss\n",
    "\n",
    "model.eval()\n",
    "output = list()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(data_test):\n",
    "        fps = torch.tensor(batch[0], device=args.device)\n",
    "        toxs = torch.tensor(batch[1], device=args.device)\n",
    "\n",
    "        pred_toxs = model(fps)\n",
    "\n",
    "        output.append([0 if x<0.5 else 1 for x in pred_toxs])\n",
    "\n",
    "    output = np.concatenate(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
