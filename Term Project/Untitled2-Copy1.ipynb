{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.max_mol = 11100\n",
    "args.max_peaks = 150\n",
    "args.max_atoms = 250\n",
    "args.max_partial_charge = 4.0\n",
    "args.min_partial_charge = -1.0\n",
    "args.max_intensity = 1000\n",
    "args.min_intensity = 0\n",
    "args.max_shift = -3\n",
    "args.min_shift = 20\n",
    "args.num_feature = 59\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nmrDB_pickle(file_name, num_mol, max_peak):\n",
    "    df = pd.read_pickle(file_name).head(num_mol)\n",
    "    \n",
    "    inchi_list = df['inchi'].tolist()\n",
    "    peaks_list = df['peaks'].tolist()\n",
    "    \n",
    "    for i, inchi in enumerate(inchi_list):\n",
    "        if '\\n' in inchi:\n",
    "            inchi_list[i] = inchi.split('\\n')[0]\n",
    "    \n",
    "    inchi_list_final = list()\n",
    "    shift_list = list()\n",
    "    intensity_list = list()\n",
    "    num_peak_list = list()\n",
    "    for idx, peaks in enumerate(peaks_list):\n",
    "        isValid = True\n",
    "        shifts = [0] * max_peak\n",
    "        intensities = [0] * max_peak\n",
    "        for i, peak in enumerate(peaks):\n",
    "            shifts[i] = float(peak[1])\n",
    "            intensities[i] = float(peak[2])\n",
    "            if intensities[i] < 0:\n",
    "                isValid = False\n",
    "        if isValid:\n",
    "            shift_list.append(shifts)\n",
    "            intensity_list.append(intensities)\n",
    "            num_peak_list.append(len(peaks))\n",
    "            inchi_list_final.append(inchi_list[idx])\n",
    "        \n",
    "    return inchi_list_final, shift_list, intensity_list, num_peak_list\n",
    "\n",
    "def normalize_peak(shift_list, intensity_list, args):\n",
    "    for i in range(len(shift_list)):\n",
    "        for j in range(len(shift_list[i])):\n",
    "            shift = shift_list[i][j]\n",
    "            intensity = intensity_list[i][j]\n",
    "            shift_list[i][j] = (shift-args.min_shift)/(args.max_shift-args.min_shift)\n",
    "            intensity_list[i][j] = (intensity-args.min_intensity)/(args.max_intensity-args.min_intensity)\n",
    "\n",
    "def convert_inchi_to_graph(inchi_list, max_atoms, num_feature):\n",
    "    adj = list()\n",
    "    adj_norm = list()\n",
    "    features = list()\n",
    "    for inchi in inchi_list:\n",
    "        # Generate mol from InChI code.\n",
    "        iMol = Chem.inchi.MolFromInchi(inchi)\n",
    "        # Add H atoms to the mol.\n",
    "        iMol = Chem.rdmolops.AddHs(iMol)\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        if (iAdjTmp.shape[0] <= max_atoms):\n",
    "            # Preprocess features\n",
    "            iFeature = np.zeros((max_atoms, num_feature))\n",
    "            iFeatureTmp = []\n",
    "            AllChem.ComputeGasteigerCharges(iMol)\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append(atom_feature(atom))\n",
    "            iFeature[0:len(iFeatureTmp), 0:num_feature] = iFeatureTmp\n",
    "            features.append(iFeature)\n",
    "            # Preprocess adjacency matrix\n",
    "            iAdj = np.zeros((max_atoms, max_atoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "    \n",
    "    return features, adj\n",
    "\n",
    "def normalized_partial_charge_of_atom(atom):\n",
    "    partial_charge = float(atom.GetProp(\"_GasteigerCharge\"))\n",
    "    partial_charge = (partial_charge-args.min_partial_charge)/(args.max_partial_charge-args.min_partial_charge)\n",
    "    return partial_charge\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()] +\n",
    "                    [normalized_partial_charge_of_atom(atom)])    # (40, 6, 5, 6, 1, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_feature, list_adj, list_shift, list_intensity, list_num_peak):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_shift = list_shift\n",
    "        self.list_intensity = list_intensity\n",
    "        self.list_num_peak = list_num_peak\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.list_feature[index]\n",
    "        adj = self.list_adj[index]\n",
    "        shift = self.list_shift[index]\n",
    "        intensity = self.list_intensity[index]\n",
    "        num_peak = self.list_num_peak[index]\n",
    "        return feature, adj, shift, intensity, num_peak\n",
    "    \n",
    "class NumPeakDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_feature, list_adj, list_num_peak):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_num_peak = list_num_peak\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.list_feature[index]\n",
    "        adj = self.list_adj[index]\n",
    "        num_peak = self.list_num_peak[index]\n",
    "        return feature, adj, num_peak\n",
    "    \n",
    "def partition(list_feature, list_adj, list_shift, list_intensity, list_num_peak, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "    \n",
    "    list_shift = np.asarray(list_shift)\n",
    "    list_intensity = np.asarray(list_intensity)\n",
    "    list_num_peak = np.asarray(list_num_peak)\n",
    "    \n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    shift_train = list_shift[:num_train]\n",
    "    intensity_train = list_intensity[:num_train]\n",
    "    num_peak_train = list_num_peak[:num_train]\n",
    "    \n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    shift_val = list_shift[num_train:num_train + num_val]\n",
    "    intensity_val = list_intensity[num_train:num_train + num_val]\n",
    "    num_peak_val = list_num_peak[num_train:num_train + num_val]\n",
    "\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    shift_test = list_shift[num_total - num_test:]\n",
    "    intensity_test = list_intensity[num_total - num_test:]\n",
    "    num_peak_test = list_num_peak[num_total - num_test:]\n",
    "\n",
    "    train_set = NMRDataset(feature_train, adj_train, shift_train, intensity_train, num_peak_train)\n",
    "    val_set = NMRDataset(feature_val, adj_val, shift_val, intensity_val, num_peak_val)\n",
    "    test_set = NMRDataset(feature_test, adj_test, shift_test, intensity_test, num_peak_test)\n",
    "    \n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition\n",
    "\n",
    "def partition_num_peak(list_feature, list_adj, list_num_peak, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "    \n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    num_peak_train = list_num_peak[:num_train]\n",
    "    \n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    num_peak_val = list_num_peak[num_train:num_train + num_val]\n",
    "    \n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    num_peak_test = list_num_peak[num_total - num_test:]\n",
    "    \n",
    "    train_set = NumPeakDataset(feature_train, adj_train, num_peak_train)\n",
    "    val_set = NumPeakDataset(feature_val, adj_val, num_peak_val)\n",
    "    test_set = NumPeakDataset(feature_test, adj_test, num_peak_test)\n",
    "    \n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inchi, list_shift, list_intensity, list_num_peak = read_nmrDB_pickle('nmrDB_deduplicated.pkl', args.max_mol, args.max_peaks)\n",
    "list_feature, list_adj = convert_inchi_to_graph(list_inchi, args.max_atoms, args.num_feature)\n",
    "normalize_peak(list_shift, list_intensity, args)\n",
    "dict_partition = partition(list_feature, list_adj, list_shift, list_intensity, list_num_peak, args)\n",
    "#dict_partition_num_peak = partition_num_peak(list_feature, list_adj, list_num_peak, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11098 11098 11098\n"
     ]
    }
   ],
   "source": [
    "print(len(list_feature), len(list_adj), len(list_num_peak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x, out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, output_dim, num_head):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.atn_dim = output_dim // num_head\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        self.corelations = nn.ParameterList()\n",
    "        for i in range(self.num_head):\n",
    "            self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
    "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim)\n",
    "            nn.init.xavier_uniform_(corelation)\n",
    "            self.corelations.append(nn.Parameter(corelation))\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            x_transformed = self.linears[i](x)\n",
    "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
    "            x_head = torch.matmul(alpha, x_transformed)\n",
    "            heads.append(x_head)\n",
    "        output = torch.cat(heads, dim=2)\n",
    "        return output\n",
    "            \n",
    "    def attention_matrix(self, x_transformed, corelation, adj):\n",
    "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
    "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
    "        alpha = torch.mul(alpha, adj)\n",
    "        alpha = self.tanh(alpha)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False, atn=False, num_head=1, dropout=0):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.use_atn = atn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.attention = Attention(out_dim, out_dim, num_head)\n",
    "        self.activation = act\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout2d(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        if self.use_atn:\n",
    "            out = self.attention(out, adj)\n",
    "        else:\n",
    "            out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, atn=True, num_head=1, sc='gsc', dropout=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                        bn,\n",
    "                                        atn,\n",
    "                                        num_head,\n",
    "                                        dropout))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.atn,\n",
    "                                        args.num_head,\n",
    "                                        args.sc,\n",
    "                                        args.dropout))\n",
    "        self.readout = ReadOut(args.hidden_dim, \n",
    "                               args.readout_dim,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred1 = Predictor(args.readout_dim,\n",
    "                               args.shift_pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred2 = Predictor(args.shift_pred_dim1,\n",
    "                               args.shift_pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred3 = Predictor(args.shift_pred_dim2,\n",
    "                               args.out_dim)\n",
    "        self.intensity_pred1 = Predictor(args.readout_dim,\n",
    "                                         args.intensity_pred_dim1,\n",
    "                                         act=nn.ReLU())\n",
    "        self.intensity_pred2 = Predictor(args.intensity_pred_dim1,\n",
    "                                         args.intensity_pred_dim2,\n",
    "                                         act=nn.ReLU())\n",
    "        self.intensity_pred3 = Predictor(args.intensity_pred_dim2,\n",
    "                                         args.out_dim)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        shift_out = self.shift_pred1(out)\n",
    "        shift_out = self.shift_pred2(shift_out)\n",
    "        shift_out = self.shift_pred3(shift_out)\n",
    "        intensity_out = self.intensity_pred1(out)\n",
    "        intensity_out = self.intensity_pred2(intensity_out)\n",
    "        intensity_out = self.intensity_pred3(intensity_out)\n",
    "        return shift_out, intensity_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Validate, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, criterion, data_train, bar, args):\n",
    "    epoch_train_loss = 0\n",
    "    new_list_shift = []\n",
    "    new_list_intensity = []\n",
    "    new_pred_list_shift = []\n",
    "    new_pred_list_intensity = []\n",
    "    for i, batch in enumerate(data_train):\n",
    "        # [batch_size, max_atom, num_feature], [100, 250, 59]\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        # [batch_size, max_peak]\n",
    "        # [150, 100]\n",
    "        #print(len(batch[2]), len(batch[2][0]))\n",
    "        list_shift = torch.tensor(batch[2]).to(device).float()\n",
    "        list_intensity = torch.tensor(batch[3]).to(device).float()\n",
    "        list_num_peak = torch.tensor(batch[4]).to(device)\n",
    "        #print(list_feature.shape, list_adj.shape, list_shift.shape, list_intensity.shape, list_num_peak.shape)\n",
    "        '''list_shift = list_shift.view(-1,1)\n",
    "        list_intensity = list_intensity.view(-1,1)''' \n",
    "        print(list_intensity.shape[0])\n",
    "        for i in range(list_intensity.shape[0]):\n",
    "            if i == 64:\n",
    "                print('Shibal')\n",
    "            new_list_shift.append(list_shift[i][0:list_num_peak[i]])\n",
    "            new_list_intensity.append(list_intensity[i][0:list_num_peak[i]])\n",
    "        list_shift = torch.cat(new_list_shift, dim = 0)\n",
    "        list_intensity = torch.cat(new_list_intensity, dim = 0)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "        list_pred_shift.detach_()\n",
    "        list_pred_intensity.detach_()\n",
    "        list_pred_shift.require_grad = True\n",
    "        list_pred_intensity.require_grad = True\n",
    "        \n",
    "        '''list_pred_shift = list_pred_shift.view(-1, 1)\n",
    "        list_pred_intensity = list_pred_intensity.view(-1, 1)'''\n",
    "        print(len(list_pred_shift), len(list_pred_shift[0]))\n",
    "        for i in range(list_pred_intensity.shape[0]):\n",
    "            new_pred_list_shift.append(list_pred_shift[i][0:list_num_peak[i]])\n",
    "            new_pred_list_intensity.append(list_pred_intensity[i][0:list_num_peak[i]])\n",
    "        list_pred_shift = torch.cat(new_pred_list_shift, dim = 0)\n",
    "        list_pred_intensity = torch.cat(new_pred_list_intensity, dim = 0)\n",
    "        train_shift_loss = criterion(list_pred_shift, list_shift)\n",
    "        train_intensity_loss = criterion(list_pred_intensity, list_intensity)\n",
    "        \n",
    "        train_loss = train_shift_loss + train_intensity_loss\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_train_loss /= len(data_train)\n",
    "    \n",
    "    return model, epoch_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, optimizer, criterion, data_val, bar, args):\n",
    "    epoch_val_loss = 0\n",
    "    new_list_shift = []\n",
    "    new_list_intensity = []\n",
    "    new_pred_list_shift = []\n",
    "    new_pred_list_intensity = []\n",
    "    for i, batch in enumerate(data_val):\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        list_shift = torch.tensor(batch[2]).to(device).float()\n",
    "        list_intensity = torch.tensor(batch[3]).to(device).float()\n",
    "        list_num_peak = torch.tensor(batch[4]).to(device)\n",
    "        #print(len(list_num_peak))\n",
    "        '''list_shift = list_shift.view(-1,1)\n",
    "        list_intensity = list_intensity.view(-1,1)'''      \n",
    "        for i in range(list_intensity.shape[0]):\n",
    "            new_list_shift.append(list_shift[i][0:list_num_peak[i]])\n",
    "            new_list_intensity.append(list_intensity[i][0:list_num_peak[i]])\n",
    "        list_shift = torch.cat(new_list_shift, dim = 0)\n",
    "        list_intensity = torch.cat(new_list_intensity, dim = 0)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "        list_pred_shift.require_grad = False\n",
    "        list_pred_intensity.require_grad = False\n",
    "        \n",
    "        '''list_pred_shift = list_pred_shift.view(-1, 1)\n",
    "        list_pred_intensity = list_pred_intensity.view(-1, 1)'''\n",
    "        for i in range(list_pred_intensity.shape[0]):\n",
    "            new_pred_list_shift.append(list_pred_shift[i][0:list_num_peak[i]])\n",
    "            new_pred_list_intensity.append(list_pred_intensity[i][0:list_num_peak[i]])\n",
    "        list_pred_shift = torch.cat(new_pred_list_shift, dim = 0)\n",
    "        list_pred_intensity = torch.cat(new_pred_list_intensity, dim = 0)\n",
    "        \n",
    "        val_shift_loss = criterion(list_pred_shift, list_shift)\n",
    "        val_intensity_loss = criterion(list_pred_intensity, list_intensity)\n",
    "        \n",
    "        val_loss = val_shift_loss + val_intensity_loss\n",
    "        epoch_val_loss += val_loss.item()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_val_loss /= len(data_val)\n",
    "    \n",
    "    return model, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, data_test, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        shift_total = list()\n",
    "        pred_shift_total = list()\n",
    "        intensity_total = list()\n",
    "        pred_intensity_total = list()\n",
    "        for i, batch in enumerate(data_test):\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            list_shift = torch.tensor(batch[2]).to(device).float()\n",
    "            list_intensity = torch.tensor(batch[3]).to(device).float()\n",
    "            #shape [bs x max atom x num feature], [bs x max atom x max atom]\n",
    "            #shape [bs x num peaks], [bs x num peaks]\n",
    "            #print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape)\n",
    "            list_num_peak = torch.tensor(batch[4]).to(device).float()\n",
    "            list_shift = list_shift.view(-1,1)\n",
    "            list_intensity = list_intensity.view(-1,1)\n",
    "            shift_total.append(list_shift.tolist())\n",
    "            intensity_total.append(list_intensity.tolist())\n",
    "\n",
    "            list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "            #shape [bs x num peaks], [bs x num peaks]\n",
    "            #print(list_pred_shift.shape, list_pred_intensity.shape)\n",
    "            pred_shift_total.append(list_pred_shift.view(-1, 1).tolist())\n",
    "            pred_intensity_total.append(list_pred_intensity.view(-1, 1).tolist())\n",
    "        \n",
    "        mae_shift = 0\n",
    "        mae_intensity = 0\n",
    "        #print(shift_total[0])\n",
    "        #print(pred_shift_total[0])\n",
    "        for i in range(len(shift_total)):\n",
    "            mae_shift += mean_absolute_error(shift_total[i], pred_shift_total[i])/len(shift_total[i])\n",
    "            mae_intensity += mean_absolute_error(intensity_total[i], pred_intensity_total[i])/len(intensity_total[i])\n",
    "        mae_shift /= len(shift_total)\n",
    "        mae_intensity /= len(intensity_total)\n",
    "        print(mae_shift, mae_intensity)\n",
    "        \n",
    "    return mae_shift, mae_intensity, shift_total, pred_shift_total, intensity_total, pred_intensity_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dict_partition, device, bar, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    model = GCNNet(args)\n",
    "    model.to(device)\n",
    "    \n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, 'Undefined Optimizer Type'\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    \n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "\n",
    "    data_train = DataLoader(dict_partition['train'], \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=True)\n",
    "\n",
    "    data_val = DataLoader(dict_partition['val'],\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=False)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        model, train_loss = train(model, device, optimizer, criterion, data_train, bar, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "        \n",
    "        model, val_loss = validate(model, device, optimizer, criterion, data_val, bar, args)\n",
    "        list_val_loss.append(val_loss)\n",
    "        \n",
    "    data_test = DataLoader(dict_partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=args.shuffle)\n",
    "\n",
    "    mae_shift, mae_intensity, shift_total, pred_shift_total, intensity_total, pred_intensity_total = test(model, device, data_test, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_val_loss = list_val_loss\n",
    "    args.shift_total = shift_total\n",
    "    args.pred_shift_total = pred_shift_total\n",
    "    args.intensity_total = intensity_total\n",
    "    args.pred_intensity_total = pred_intensity_total\n",
    "    args.mae_shift = mae_shift\n",
    "    args.mae_intensity = mae_intensity\n",
    "    args.time_required = time_required\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(df_result, var1, var2):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    df_mae = df_result.pivot(var1, var2, 'mae')\n",
    "    df_std = df_result.pivot(var1, var2, 'std')\n",
    "    df_mae = df_mae[df_mae.columns].astype(float)\n",
    "    df_std = df_std[df_std.columns].astype(float)\n",
    "    \n",
    "    hm_mae = sns.heatmap(df_mae, ax=ax[0], annot=True, fmt='f', linewidths=.5, cmap='YlGnBu')\n",
    "    hm_std = sns.heatmap(df_std, ax=ax[1], annot=True, fmt='f', linewidths=.5, cmap='YlGnBu')\n",
    "    \n",
    "    fig.suptitle('Performance depends on ' + var1 + ' vs ' + var2)\n",
    "    hm_mae.set_title('MAE depends on ' + var1 + ' vs ' + var2)\n",
    "    hm_std.set_title('Std depends on ' + var1 + ' vs ' + var2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_bar(df_result, var1, var2):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    bar_mae = sns.barplot(x=var1, y='mae', hue=var2, data=df_result, ax=ax[0])\n",
    "    bar_std = sns.barplot(x=var1, y='std', hue=var2, data=df_result, ax=ax[1])\n",
    "    \n",
    "    bar_mae.set_title('MAE depends on ' + var1 + ' vs ' + var2)\n",
    "    bar_std.set_title('Std depends on ' + var1 + ' vs ' + var2)\n",
    "    fig.suptitle('Performance depends on ' + var1 + ' vs ' + var2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(df_result, var1, var2, ylim):\n",
    "    def plot(x, ylim=1.0, **kwargs):\n",
    "        plt.plot(x[0], **kwargs)\n",
    "        plt.ylim(0.0, ylim)\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    g = sns.FacetGrid(df_result, row=var1, col=var2, margin_titles=True)\n",
    "    g.map(plot, 'list_train_loss', ylim=ylim, label='Train Loss')\n",
    "    g.map(plot, 'list_val_loss', ylim=ylim, color='r', label='Validation Loss')\n",
    "    g.fig.suptitle('Loss vs Epochs depends on ' + var1 + ' vs ' + var2, size=16)\n",
    "    g.fig.subplots_adjust(top=.9)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df_result, var1, var2):\n",
    "    def scatter(x, y, **kwargs):\n",
    "        plt.scatter(x[0], y[0], alpha=0.3, s=2)\n",
    "    def identity(x, y, **kwargs):\n",
    "        plt.plot(x[0], x[0], alpha=0.4, color='black')\n",
    "    \n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    g = sns.FacetGrid(df_result, row=var1, col=var2, margin_titles=True)\n",
    "    g.map(scatter, 'logP_total', 'pred_logP_total')\n",
    "    g.map(identity, 'logP_total', 'logP_total')\n",
    "    g.fig.suptitle('Truth Distribution depends on ' + var1 + ' vs ' + var2, size=16)\n",
    "    g.fig.subplots_adjust(top=.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.lr = 0.0001\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'Adam'\n",
    "args.epoch = 1\n",
    "args.n_block = 2\n",
    "args.n_layer = 2\n",
    "args.n_atom = args.max_atoms\n",
    "args.in_dim = args.num_feature\n",
    "args.hidden_dim = 64\n",
    "args.readout_dim = 256\n",
    "args.shift_pred_dim1 = 256\n",
    "args.shift_pred_dim2 = 128\n",
    "args.intensity_pred_dim1 = 256\n",
    "args.intensity_pred_dim2 = 128\n",
    "args.out_dim = args.max_peaks\n",
    "args.bn = True\n",
    "args.sc = 'gsc'\n",
    "args.atn = True\n",
    "args.num_head = 16\n",
    "args.dropout = 0.1\n",
    "args.step_size = 10\n",
    "args.gamma = 0.1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c6d4d4a55145b0984c6ea6c9a294a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9987), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64 150\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6a8cbfb99cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" took \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_required\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdict_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-aa0efc8ceb54>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dict_partition, device, bar, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mlist_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-532dca2a2e27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, criterion, data_train, bar, args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "list_lr = [0.01]\n",
    "list_n_block = [2]\n",
    "var1 = \"lr\"\n",
    "var2 = \"n_block\"\n",
    "\n",
    "dict_result = dict()\n",
    "n_iter = len(list_n_block)*len(list_lr)*args.epoch*(len(dict_partition['train'])+len(dict_partition['val']))\n",
    "bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n",
    "\n",
    "for lr in list_lr:\n",
    "    for n_block in list_n_block:\n",
    "        args.lr = lr\n",
    "        args.n_block = n_block\n",
    "        args.exp_name = var1+':'+str(lr)+'/'+var2+':'+str(n_block)\n",
    "        result = vars(experiment(dict_partition, device, bar, args))\n",
    "        print(args.exp_name + \" took \" + str(int(args.time_required)) + \"seconds.\")\n",
    "        dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "bar.close()\n",
    "\n",
    "df_result = pd.DataFrame(dict_result).transpose()\n",
    "df_result.to_json('lr vs n_block.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mae'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-595841986d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr vs n_block.JSON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot_performance_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-5f811050133e>\u001b[0m in \u001b[0;36mplot_performance\u001b[0;34m(df_result, var1, var2)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_mae\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m   5192\u001b[0m         \"\"\"\n\u001b[1;32m   5193\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5196\u001b[0m     _shared_docs['pivot_table'] = \"\"\"\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m    411\u001b[0m                                         columns=values)\n\u001b[1;32m    412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             indexed = self._constructor_sliced(self[values].values,\n\u001b[0m\u001b[1;32m    414\u001b[0m                                                index=index)\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mae'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = pd.read_json('lr vs n_block.JSON', orient='table')\n",
    "\n",
    "plot_performance(df_result, var1, var2)\n",
    "plot_performance_bar(df_result, var1, var2)\n",
    "plot_loss(df_result, var1, var2, 1000)\n",
    "plot_distribution(df_result, var1, var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  21.2289199829,\n",
       "  ...]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['pred_logP_total'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
