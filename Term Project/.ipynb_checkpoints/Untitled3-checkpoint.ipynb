{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.max_mol = 11100\n",
    "args.max_peaks = 150\n",
    "args.max_atoms = 250\n",
    "args.max_partial_charge = 4.0\n",
    "args.min_partial_charge = -1.0\n",
    "args.max_intensity = 1000\n",
    "args.min_intensity = 0\n",
    "args.max_shift = 20\n",
    "args.min_shift = -3\n",
    "args.resolution = 100\n",
    "args.num_feature = 59\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nmrDB_pickle(file_name, num_mol, max_peak):    \n",
    "    df = pd.read_pickle(file_name)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df = df.head(num_mol)\n",
    "    \n",
    "    inchi_list = df['inchi'].tolist()\n",
    "    peaks_list = df['peaks'].tolist()\n",
    "    \n",
    "    for i, inchi in enumerate(inchi_list):\n",
    "        if '\\n' in inchi:\n",
    "            inchi_list[i] = inchi.split('\\n')[0]\n",
    "    \n",
    "    inchi_list_final = list()\n",
    "    shift_list = list()\n",
    "    intensity_list = list()\n",
    "    num_peak_list = list()\n",
    "    for idx, peaks in enumerate(peaks_list):\n",
    "        isValid = True\n",
    "        shifts = list()\n",
    "        intensities = list()\n",
    "        for i, peak in enumerate(peaks):\n",
    "            shifts.append(float(peak[1]))\n",
    "            intensities.append(float(peak[2]))\n",
    "            if intensities[i] < 0:\n",
    "                isValid = False\n",
    "        if sum(intensities) == 0:\n",
    "            isValid = False\n",
    "        if isValid:\n",
    "            shift_list.append(shifts)\n",
    "            intensity_list.append(intensities)\n",
    "            num_peak_list.append(len(peaks))\n",
    "            inchi_list_final.append(inchi_list[idx])\n",
    "        \n",
    "    return inchi_list_final, shift_list, intensity_list, num_peak_list\n",
    "\n",
    "def convert_to_distribution(list_shift, list_intensity, resolution):\n",
    "    list_distribution = list()\n",
    "    \n",
    "    for i in range(len(list_shift)):\n",
    "        distribution = [0] * resolution\n",
    "        scope = (args.max_shift-args.min_shift)/resolution\n",
    "\n",
    "        for j in range(len(list_shift[i])):\n",
    "            shift = list_shift[i][j]\n",
    "            intensity = list_intensity[i][j]\n",
    "            idx = int((shift-args.min_shift)/scope)\n",
    "            distribution[idx] += intensity\n",
    "        total_prob = sum(distribution)\n",
    "        distribution = [ prob/total_prob for prob in distribution ]\n",
    "        \n",
    "        list_distribution.append(distribution)\n",
    "    \n",
    "    return np.asarray(list_distribution)\n",
    "\n",
    "def convert_inchi_to_graph(inchi_list, max_atoms, num_feature):\n",
    "    adj = list()\n",
    "    adj_norm = list()\n",
    "    features = list()\n",
    "    for inchi in inchi_list:\n",
    "        # Generate mol from InChI code.\n",
    "        iMol = Chem.inchi.MolFromInchi(inchi)\n",
    "        # Add H atoms to the mol.\n",
    "        iMol = Chem.rdmolops.AddHs(iMol)\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        if (iAdjTmp.shape[0] <= max_atoms):\n",
    "            # Preprocess features\n",
    "            iFeature = np.zeros((max_atoms, num_feature))\n",
    "            iFeatureTmp = []\n",
    "            AllChem.ComputeGasteigerCharges(iMol)\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append(atom_feature(atom))\n",
    "            iFeature[0:len(iFeatureTmp), 0:num_feature] = iFeatureTmp\n",
    "            features.append(iFeature)\n",
    "            # Preprocess adjacency matrix\n",
    "            iAdj = np.zeros((max_atoms, max_atoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "    \n",
    "    return features, adj\n",
    "\n",
    "def normalized_partial_charge_of_atom(atom):\n",
    "    partial_charge = float(atom.GetProp(\"_GasteigerCharge\"))\n",
    "    partial_charge = (partial_charge-args.min_partial_charge)/(args.max_partial_charge-args.min_partial_charge)\n",
    "    return partial_charge\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()] +\n",
    "                    [normalized_partial_charge_of_atom(atom)])    # (40, 6, 5, 6, 1, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_feature, list_adj, list_distribution):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_distribution = list_distribution\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.list_feature[index]\n",
    "        adj = self.list_adj[index]\n",
    "        distribution = self.list_distribution[index]\n",
    "        return feature, adj, distribution\n",
    "    \n",
    "def partition(list_feature, list_adj, list_distribution, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "    \n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    distribution_train = list_distribution[:num_train]\n",
    "    \n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    distribution_val = list_distribution[num_train:num_train + num_val]\n",
    "    \n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    distribution_test = list_distribution[num_total - num_test:]\n",
    "    \n",
    "    train_set = NMRDataset(feature_train, adj_train, distribution_train)\n",
    "    val_set = NMRDataset(feature_val, adj_val, distribution_val)\n",
    "    test_set = NMRDataset(feature_test, adj_test, distribution_test)\n",
    "    \n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inchi, list_shift, list_intensity, list_num_peak = read_nmrDB_pickle('nmrDB_deduplicated.pkl', args.max_mol, args.max_peaks)\n",
    "list_feature, list_adj = convert_inchi_to_graph(list_inchi, args.max_atoms, args.num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_distribution = convert_to_distribution(list_shift, list_intensity, args.resolution)\n",
    "dict_partition = partition(list_feature, list_adj, list_distribution, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x, out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, output_dim, num_head):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.atn_dim = output_dim // num_head\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        self.corelations = nn.ParameterList()\n",
    "        for i in range(self.num_head):\n",
    "            self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
    "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim)\n",
    "            nn.init.xavier_normal_(corelation)\n",
    "            self.corelations.append(nn.Parameter(corelation))\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            x_transformed = self.linears[i](x)\n",
    "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
    "            x_head = torch.matmul(alpha, x_transformed)\n",
    "            heads.append(x_head)\n",
    "        output = torch.cat(heads, dim=2)\n",
    "        return output\n",
    "            \n",
    "    def attention_matrix(self, x_transformed, corelation, adj):\n",
    "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
    "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
    "        alpha = torch.mul(alpha, adj)\n",
    "        alpha = self.tanh(alpha)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False, atn=False, num_head=1, dropout=0):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.use_atn = atn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.attention = Attention(out_dim, out_dim, num_head)\n",
    "        self.activation = act\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout2d(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        if self.use_atn:\n",
    "            out = self.attention(out, adj)\n",
    "        else:\n",
    "            out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, atn=True, num_head=1, sc='gsc', dropout=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                        bn,\n",
    "                                        atn,\n",
    "                                        num_head,\n",
    "                                        dropout))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMRNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(NMRNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.atn,\n",
    "                                        args.num_head,\n",
    "                                        args.sc,\n",
    "                                        args.dropout))\n",
    "        self.readout = ReadOut(args.hidden_dim, \n",
    "                               args.pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred1 = Predictor(args.pred_dim1,\n",
    "                               args.pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred2 = Predictor(args.pred_dim2,\n",
    "                               args.pred_dim3,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred3 = Predictor(args.pred_dim3,\n",
    "                               args.out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        out = self.log_normalize(out)\n",
    "        return out\n",
    "    \n",
    "    def log_normalize(self, x):\n",
    "        #batch_size, resolution = x.shape\n",
    "        x = self.sigmoid(x)\n",
    "        total_probs = torch.sum(x, dim=1)\n",
    "        x = torch.t(x)\n",
    "        x = torch.div(x, total_probs)\n",
    "        x = torch.t(x)\n",
    "        return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, criterion, data_train, bar, args):\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        # [batch_size, max_atoms, num_feature]\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        # [batch_size, max_atoms, max_atoms]\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        # [batch_size, resolution]\n",
    "        list_distribution = torch.tensor(batch[2]).to(device).float()\n",
    "                \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # [batch_size, resolution]\n",
    "        list_pred_distribution = model(list_feature, list_adj)\n",
    "        list_pred_distribution.require_grad = False\n",
    "        \n",
    "        train_loss = criterion(list_pred_distribution, list_distribution)\n",
    "        #train_loss = criterion(torch.add(list_pred_distribution, 1), torch.add(list_distribution, 1))\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_train_loss /= len(data_train)\n",
    "    \n",
    "    return model, epoch_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, criterion, data_val, bar, args):\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_val):\n",
    "            # [batch_size, max_atoms, num_feature]\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            # [batch_size, max_atoms, max_atoms]\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            # [batch_size, resolution]\n",
    "            list_distribution = torch.tensor(batch[2]).to(device).float()\n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            # [batch_size, resolution]\n",
    "            list_pred_distribution = model(list_feature, list_adj)\n",
    "            list_pred_distribution.require_grad = False\n",
    "            \n",
    "            val_loss = criterion(list_pred_distribution, list_distribution)\n",
    "            #val_loss = criterion(torch.add(list_pred_distribution, 1), torch.add(list_distribution, 1))\n",
    "            epoch_val_loss += val_loss.item()\n",
    "            print(val_loss.item())\n",
    "            \n",
    "            bar.update(len(list_feature))\n",
    "\n",
    "    epoch_val_loss /= len(data_val)\n",
    "    \n",
    "    return model, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, data_test, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        klDiv = 0\n",
    "        klDivLoss = nn.KLDivLoss()\n",
    "        # [len_data_test, resolution]\n",
    "        distribution_total = list()\n",
    "        pred_distribution_total = list()\n",
    "        for i, batch in enumerate(data_test):\n",
    "            # [batch_size, max_atoms, num_feature]\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            # [batch_size, max_atoms, max_atoms]\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            # [batch_size, resolution]\n",
    "            list_distribution = torch.tensor(batch[2]).to(device).float()\n",
    "            \n",
    "            # [batch_size, resolution]\n",
    "            list_pred_distribution = model(list_feature, list_adj)\n",
    "            \n",
    "            klDiv += klDivLoss(list_pred_distribution, list_distribution)\n",
    "            #klDiv += klDivLoss(torch.add(list_pred_distribution, 1), torch.add(list_distribution, 1))\n",
    "            for i in range(len(list_distribution)):\n",
    "                distribution_total.append(list_distribution[i].tolist())\n",
    "                pred_distribution_total.append(torch.exp(list_pred_distribution[i]).tolist())\n",
    "            \n",
    "        klDiv /= len(data_test)\n",
    "        klDiv = klDiv.item()\n",
    "        \n",
    "    return klDiv, distribution_total, pred_distribution_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dict_partition, device, bar, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    model = NMRNet(args)\n",
    "    model.to(device)\n",
    "        \n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == \"Adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters(),\n",
    "                                   lr=args.lr,\n",
    "                                   weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, 'Undefined Optimizer Type'\n",
    "        \n",
    "    criterion = nn.KLDivLoss(reduce=False)\n",
    "    if args.optim != \"Adadelta\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "\n",
    "    data_train = DataLoader(dict_partition['train'], \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=args.shuffle)\n",
    "\n",
    "    data_val = DataLoader(dict_partition['val'],\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=args.shuffle)\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        if args.optim != \"Adadelta\":\n",
    "            scheduler.step()\n",
    "        model, train_loss = train(model, device, optimizer, criterion, data_train, bar, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "        \n",
    "        model, val_loss = validate(model, device, criterion, data_val, bar, args)\n",
    "        list_val_loss.append(val_loss)\n",
    "\n",
    "    data_test = DataLoader(dict_partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=args.shuffle)\n",
    "\n",
    "    klDiv, distribution_total, pred_distribution_total = test(model, device, data_test, args)\n",
    "        \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_val_loss = list_val_loss\n",
    "    args.distribution_total = np.asarray(distribution_total)\n",
    "    args.pred_distribution_total = np.asarray(pred_distribution_total)\n",
    "    args.klDiv = klDiv\n",
    "    args.time_required = time_required\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_grid(df_result, var1, var2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(df_result):\n",
    "    plt.plot(df_result['list_train_loss'][0])\n",
    "    plt.plot(df_result['list_val_loss'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 128\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.optim = 'Adadelta'\n",
    "args.epoch = 3\n",
    "args.n_block = 2\n",
    "args.n_layer = 3\n",
    "args.n_atom = args.max_atoms\n",
    "args.in_dim = args.num_feature\n",
    "args.hidden_dim = 128\n",
    "args.pred_dim1 = 256\n",
    "args.pred_dim2 = 128\n",
    "args.pred_dim3 = 256\n",
    "args.out_dim = args.resolution\n",
    "args.bn = True\n",
    "args.sc = 'gsc'\n",
    "args.atn = True\n",
    "args.num_head = 4\n",
    "args.dropout = 0.3\n",
    "args.step_size = 5\n",
    "args.gamma = 0.1\n",
    "args.clip = 0.25\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f5583a3ccd459f9b27947faf489648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29952), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039721619337797165\n",
      "0.04116344451904297\n",
      "0.03909144550561905\n",
      "0.041820332407951355\n",
      "0.039733998477458954\n",
      "0.04158499091863632\n",
      "0.040707141160964966\n",
      "0.04077807068824768\n",
      "0.03956655040383339\n",
      "0.03952057287096977\n",
      "inf\n",
      "0.03890363872051239\n",
      "0.041624221950769424\n",
      "0.03956535831093788\n",
      "0.04139433801174164\n",
      "0.04050689563155174\n",
      "0.040606752038002014\n",
      "0.03940578177571297\n",
      "0.03933504596352577\n",
      "inf\n",
      "0.03872591629624367\n",
      "0.04144052416086197\n",
      "0.039406899362802505\n",
      "0.041211262345314026\n",
      "0.04031946137547493\n",
      "0.04043624922633171\n",
      "0.0392611101269722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_result = dict()\n",
    "n_iter = args.epoch*(len(dict_partition['train'])+len(dict_partition['val']))\n",
    "bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n",
    "\n",
    "args.exp_name = \"result\"\n",
    "result = vars(experiment(dict_partition, device, bar, args))\n",
    "dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "bar.close()\n",
    "\n",
    "df_result = pd.DataFrame(dict_result).transpose()\n",
    "df_result.to_json('result.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result    [inf, inf, inf]\n",
       "Name: list_train_loss, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['list_train_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result    [0.04046306593550576, inf, inf]\n",
       "Name: list_val_loss, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['list_val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
