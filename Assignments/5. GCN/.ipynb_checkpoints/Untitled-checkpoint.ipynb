{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from utils import read_ZINC_smiles, smiles_to_onehot, partition, OneHotLogPDataSet\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.15\n",
    "args.test_size = 0.15\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19265d6fe90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi_list = []\n",
    "    logP_list = []\n",
    "\n",
    "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
    "        smi = contents[i].strip()\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "\n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "\n",
    "    return smi_list, logP_list\n",
    "\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles) < max_length:\n",
    "            smiles += \" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
    "        for i, elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total = []\n",
    "\n",
    "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj = []\n",
    "    adj_norm = []\n",
    "    features = []\n",
    "    maxNumAtoms = 50\n",
    "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        #Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
    "            # Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms, 58))\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only\n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
    "            features.append(iFeature)\n",
    "\n",
    "            # Adj-preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "\n",
    "    return features, adj\n",
    "    \n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    #print list((map(lambda s: x == s, allowable_set)))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "class GCNDataset(Dataset):\n",
    "    def __init__(self, list_feature, list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "\n",
    "\n",
    "def partition(list_feature, list_adj, list_logP, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    logP_val = list_logP[num_train:num_train + num_val]\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_train:num_train + num_val]\n",
    "    logP_test = list_logP[num_total - num_test:]\n",
    "\n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "\n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079a8d4a7c85440f8a08f8f899f653b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading Data', style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00752e9004c84e97a3ced2001adeac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting to Graph', style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles', 100)\n",
    "list_feature, list_adj = convert_to_graph(list_smi)\n",
    "args.dict_partition = partition(list_feature, list_adj, list_logP, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_x, new_x):        \n",
    "        z = self.gate_coefficient(input_x, new_x)\n",
    "        out = torch.mul(new_x, z) + torch.mul(input_x, 1.0-z)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def gate_coefficient(input_x, new_x):\n",
    "        X1 = self.linear(input_x)\n",
    "        X2 = self.linear(new_x)\n",
    "        gate_coefficient = self.sigmoid(X1 + X2)\n",
    "        \n",
    "        return gate_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, activation, sc='no'):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.sc = sc\n",
    "\n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.hidden_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.gated_skip_connection = GatedSkipConnection(self.in_dim, \n",
    "                                                         self.hidden_dim, \n",
    "                                                         self.activation)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        out = torch.matmul(adj, out)\n",
    "        \n",
    "        if (self.sc == 'gsc'):\n",
    "            out = self.gated_skip_connection(x, out)\n",
    "        elif (self.sc == 'no'):\n",
    "            out = self.activation(out)\n",
    "        else:\n",
    "            out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, activation):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, activation=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_layer, \n",
    "                 in_dim, \n",
    "                 hidden_dim_1, \n",
    "                 hidden_dim_2,\n",
    "                 out_dim,\n",
    "                 sc='no'):\n",
    "        super(LogPPredictor, self).__init__()\n",
    "        \n",
    "        self.n_layer = n_layer\n",
    "        self.graph_convolution_1 = GraphConvolution(in_dim, hidden_dim_1, nn.ReLU(), sc)\n",
    "        self.graph_convolution_2 = GraphConvolution(hidden_dim_1, hidden_dim_1, nn.ReLU(), sc)\n",
    "        self.readout = ReadOut(hidden_dim_1, hidden_dim_2, nn.Sigmoid())\n",
    "        self.predictor_1 = Predictor(hidden_dim_2, hidden_dim_2, nn.ReLU())\n",
    "        self.predictor_2 = Predictor(hidden_dim_2, hidden_dim_2, nn.Tanh())\n",
    "        self.predictor_3 = Predictor(hidden_dim_2, out_dim)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        out = self.graph_convolution_1(x, adj)\n",
    "        for i in range(self.n_layer-1):\n",
    "            out = self.graph_convolution_2(out, adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.predictor_1(out)\n",
    "        out = self.predictor_2(out)\n",
    "        out = self.predictor_3(out)\n",
    "        \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 10\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.optim = optim.Adam\n",
    "args.criterion = nn.MSELoss()\n",
    "args.epoch = 2\n",
    "args.device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogPPredictor(1, 58, 64, 128, 1, 'no')\n",
    "model.to(args.device)\n",
    "\n",
    "list_train_loss = list()\n",
    "list_val_loss = list()\n",
    "acc = 0\n",
    "mse = 0\n",
    "\n",
    "optimizer = args.optim(model.parameters(),\n",
    "                       lr=args.lr,\n",
    "                       weight_decay=args.l2_coef)\n",
    "\n",
    "data_train = DataLoader(args.dict_partition['train'], \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle)\n",
    "\n",
    "data_val = DataLoader(args.dict_partition['val'],\n",
    "                     batch_size=args.batch_size,\n",
    "                     shuffle=args.shuffle)\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        list_feature = torch.tensor(batch[0],\n",
    "                                    dtype=torch.float,\n",
    "                                    device=args.device)\n",
    "        list_adj = torch.tensor(batch[1],\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        list_logP = torch.tensor(batch[2],\n",
    "                                 dtype=torch.float,\n",
    "                                 device=args.device)\n",
    "        list_logP = list_logP.view(-1,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        list_pred_logP = model(list_feature, list_adj)\n",
    "        list_pred_logP.require_grad = False\n",
    "        train_loss = args.criterion(list_pred_logP, list_logP)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    list_train_loss.append(epoch_train_loss/len(data_train))\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_val):\n",
    "        list_feature = torch.tensor(batch[0],\n",
    "                                    dtype=torch.float,\n",
    "                                    device=args.device)\n",
    "        list_adj = torch.tensor(batch[1],\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        list_logP = torch.tensor(batch[2],\n",
    "                                 dtype=torch.float,\n",
    "                                 device=args.device)\n",
    "        list_logP = list_logP.view(-1,1)\n",
    "        \n",
    "        list_pred_logP = model(list_feature, list_adj)\n",
    "        val_loss = args.criterion(list_pred_logP, list_logP)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        \n",
    "    list_val_loss.append(epoch_val_loss/len(data_val))\n",
    "    \n",
    "data_test = DataLoader(args.dict_partition['test'],\n",
    "                       batch_size=args.batch_size,\n",
    "                       shuffle=args.shuffle)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logP_total = list()\n",
    "    pred_logP_total = list()\n",
    "    for i, batch in enumerate(data_val):\n",
    "        list_feature = torch.tensor(batch[0],\n",
    "                                    dtype=torch.float,\n",
    "                                    device=args.device)\n",
    "        list_adj = torch.tensor(batch[1],\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        list_logP = torch.tensor(batch[2],\n",
    "                                 dtype=torch.float,\n",
    "                                 device=args.device)\n",
    "        logP_total += list_logP.tolist()\n",
    "        list_logP = list_logP.view(-1,1)\n",
    "        \n",
    "        list_pred_logP = model(list_feature, list_adj)\n",
    "        \n",
    "        pred_logP_total += list_pred_logP.tolist()\n",
    "    \n",
    "    acc = accuracy_score(logP_total, pred_logP_total)\n",
    "    mse = mean_squared_error(logP_total, pred_logP_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " torch.Size([50, 58]),\n",
       " tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " torch.Size([50, 50]),\n",
       " tensor(3.0387, dtype=torch.float64),\n",
       " torch.Size([]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = list()\n",
    "adj = list()\n",
    "logP = list()\n",
    "\n",
    "data_train = DataLoader(args.dict_partition['train'],\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle)\n",
    "\n",
    "for i, batch in enumerate(data_train):\n",
    "    if i == 0:\n",
    "        feature = batch[0][0]\n",
    "        adj = batch[1][0]\n",
    "        logP = batch[2][0]\n",
    "        \n",
    "feature, feature.shape, adj, adj.shape, logP, logP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
