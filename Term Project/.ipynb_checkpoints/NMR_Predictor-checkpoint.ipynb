{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.max_mol = 11100\n",
    "args.max_peaks = 150\n",
    "args.max_atoms = 250\n",
    "args.max_partial_charge = 4.0\n",
    "args.min_partial_charge = -1.0\n",
    "args.num_feature = 59\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 134 max peaks, 23 mean peaks\n",
    "# max 209 atoms per molecule\n",
    "# total 11100 molecules\n",
    "\n",
    "def read_nmrDB_pickle(file_name, num_mol, max_peak):\n",
    "    df = pd.read_pickle(file_name).head(num_mol)\n",
    "    \n",
    "    inchi_list = df['inchi'].tolist()\n",
    "    peaks_list = df['peaks'].tolist()\n",
    "    \n",
    "    for i, inchi in enumerate(inchi_list):\n",
    "        if '\\n' in inchi:\n",
    "            inchi_list[i] = inchi.split('\\n')[0]\n",
    "    \n",
    "    shift_list = list()\n",
    "    intensity_list = list()\n",
    "    for peaks in peaks_list:\n",
    "        shifts = [0] * max_peak\n",
    "        intensities = [0] * max_peak\n",
    "        for i, peak in enumerate(peaks):\n",
    "            shifts[i] = float(peak[1])\n",
    "            intensities[i] = float(peak[2])\n",
    "        shift_list.append(shifts)\n",
    "        intensity_list.append(intensities)\n",
    "    \n",
    "    return inchi_list, shift_list, intensity_list\n",
    "\n",
    "def convert_inchi_to_graph(inchi_list, max_atoms):\n",
    "    adj = list()\n",
    "    adj_norm = list()\n",
    "    features = list()\n",
    "    for inchi in inchi_list:\n",
    "        # Generate mol from InChI code.\n",
    "        iMol = Chem.inchi.MolFromInchi(inchi)\n",
    "        # Add H atoms to the mol.\n",
    "        iMol = Chem.rdmolops.AddHs(iMol)\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        if (iAdjTmp.shape[0] <= max_atoms):\n",
    "            # Preprocess features\n",
    "            iFeature = np.zeros((max_atoms, num_feature))\n",
    "            iFeatureTmp = []\n",
    "            AllChem.ComputeGasteigerCharges(iMol)\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append(atom_feature(atom))\n",
    "            iFeature[0:len(iFeatureTmp), 0:num_feature] = iFeatureTmp\n",
    "            features.append(iFeature)\n",
    "            # Preprocess adjacency matrix\n",
    "            iAdj = np.zeros((max_atoms, max_atoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "    \n",
    "    return features, adj\n",
    "\n",
    "def normalized_partial_charge_of_atom(atom):\n",
    "    partial_charge = float(atom.GetProp(\"_GasteigerCharge\"))\n",
    "    partial_charge = (partial_charge-min_partial_charge)/(max_partial_charge-min_partial_charge)\n",
    "    return partial_charge\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()] +\n",
    "                    [normalized_partial_charge_of_atom(atom)])    # (40, 6, 5, 6, 1, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMRDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_feature, list_adj, list_shift, list_intensity):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_shift = list_shift\n",
    "        self.list_intensity = list_intensity\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.list_feature[index]\n",
    "        adj = self.list_adj[index]\n",
    "        shift = self.list_shift[index]\n",
    "        intensity = self.list_intensity[index]\n",
    "        return feature, adj, shift, intensity\n",
    "    \n",
    "def partition(list_feature, list_adj, list_shift, list_intensity, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "    \n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    shift_train = list_shift[:num_train]\n",
    "    intensity_train = list_intensity[:num_train]\n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    shift_val = list_shift[num_train:num_train + num_val]\n",
    "    intensity_val = list_intensity[num_train:num_train + num_val]\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    shift_test = list_shift[num_total - num_test:]\n",
    "    intensity_test = list_intensity[num_total - num_test:]\n",
    "    \n",
    "    train_set = NMRDataset(feature_train, adj_train, shift_train, intensity_train)\n",
    "    val_set = NMRDataset(feature_val, adj_val, shift_val, intensity_val)\n",
    "    test_set = NMRDataset(feature_test, adj_test, shift_test, intensity_test)\n",
    "    \n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_inchi, list_shift, list_intensity = read_nmrDB_pickle('nmrDB_deduplicated.pkl', 1000, args.max_peaks)\n",
    "list_feature, list_adj = convert_inchi_to_graph(list_inchi, args.max_atoms)\n",
    "dict_partition = partition(list_feature, list_adj, list_shift, list_intensity, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x, out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, output_dim, num_head):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.atn_dim = output_dim // num_head\n",
    "        \n",
    "        self.linears = nn.ModuleList()\n",
    "        self.corelations = nn.ParameterList()\n",
    "        for i in range(self.num_head):\n",
    "            self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
    "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim)\n",
    "            nn.init.xavier_uniform_(corelation)\n",
    "            self.corelations.append(nn.Parameter(corelation))\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            x_transformed = self.linears[i](x)\n",
    "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
    "            x_head = torch.matmul(alpha, x_transformed)\n",
    "            heads.append(x_head)\n",
    "        output = torch.cat(heads, dim=2)\n",
    "        return output\n",
    "            \n",
    "    def attention_matrix(self, x_transformed, corelation, adj):\n",
    "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
    "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
    "        alpha = torch.mul(alpha, adj)\n",
    "        alpha = self.tanh(alpha)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False, atn=False, num_head=1, dropout=0):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.use_atn = atn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.attention = Attention(out_dim, out_dim, num_head)\n",
    "        self.activation = act\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout2d(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        if self.use_atn:\n",
    "            out = self.attention(out, adj)\n",
    "        else:\n",
    "            out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, atn=True, num_head=1, sc='gsc', dropout=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                        bn,\n",
    "                                        atn,\n",
    "                                        num_head,\n",
    "                                        dropout))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.atn,\n",
    "                                        args.num_head,\n",
    "                                        args.sc,\n",
    "                                        args.dropout))\n",
    "        self.readout = ReadOut(args.hidden_dim, \n",
    "                               args.readout_dim,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred1 = Predictor(args.readout_dim,\n",
    "                               args.shift_pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred2 = Predictor(args.shift_pred_dim1,\n",
    "                               args.shift_pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.shift_pred3 = Predictor(args.shift_pred_dim2,\n",
    "                               args.out_dim)\n",
    "        self.intensity_pred1 = Predictor(args.readout_dim,\n",
    "                                         args.intensity_pred_dim1,\n",
    "                                         act=nn.ReLU())\n",
    "        self.intensity_pred2 = Predictor(args.intensity_pred_dim1,\n",
    "                                         args.intensity_pred_dim2,\n",
    "                                         act=nn.ReLU())\n",
    "        self.intensity_pred3 = Predictor(args.intensity_pred_dim2,\n",
    "                                         args.out_dim)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        shift_out = self.shift_pred1(out)\n",
    "        shift_out = self.shift_pred2(shift_out)\n",
    "        shift_out = self.shift_pred3(shift_out)\n",
    "        intensity_out = self.intensity_pred1(out)\n",
    "        intensity_out = self.intensity_pred2(intensity_out)\n",
    "        intensity_out = self.intensity_pred3(intensity_out)\n",
    "        return shift_out, intensity_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Validate, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, criterion, data_train, bar, args):\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        # [batch_size, max_atom, num_feature], [100, 250, 59]\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        # [batch_size, max_peak]\n",
    "        # [150, 100]\n",
    "        print(len(batch[2]), len(batch[2][0]))\n",
    "        list_shift = torch.tensor(batch[2]).to(device)\n",
    "        list_intensity = torch.tensor(batch[3]).to(device)\n",
    "        list_shift = list_shift.view(-1,1)\n",
    "        list_intensity = list_intensity.view(-1,1)        \n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "        list_pred_shift.require_grad = False\n",
    "        list_pred_intensity.require_grad = False\n",
    "        \n",
    "        train_shift_loss = criterion(list_pred_shift, list_shift)\n",
    "        train_intensity_loss = criterion(list_pred_intensity, list_intensity)\n",
    "        \n",
    "        train_loss = train_shift_loss + train_intensity_loss\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_train_loss /= len(data_train)\n",
    "    \n",
    "    return model, epoch_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, optimizer, criterion, data_val, bar, args):\n",
    "    epoch_val_loss = 0\n",
    "    for i, batch in enumerate(data_val):\n",
    "        list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "        list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "        list_shift = torch.tensor(batch[2]).to(device).float()\n",
    "        list_intensity = torch.tensor(batch[3]).to(device).float()\n",
    "        list_shift = list_shift.view(-1,1)\n",
    "        list_intensity = list_intensity.view(-1,1)        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "        list_pred_shift.require_grad = False\n",
    "        list_pred_intensity.require_grad = False\n",
    "        \n",
    "        val_shift_loss = criterion(list_pred_shift, list_shift)\n",
    "        val_intensity_loss = criterion(list_pred_intensity, list_intensity)\n",
    "        \n",
    "        val_loss = val_shift_loss + val_intensity_loss\n",
    "        epoch_val_loss += val_loss.item()\n",
    "       \n",
    "        bar.update(len(list_feature))\n",
    "\n",
    "    epoch_val_loss /= len(data_train)\n",
    "    \n",
    "    return model, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, data_test, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        shift_total = list()\n",
    "        pred_shift_total = list()\n",
    "        intensity_total = list()\n",
    "        pred_intensity_total = list()\n",
    "        for i, batch in enumerate(data_test):\n",
    "            list_feature = torch.tensor(batch[0]).to(device).float()\n",
    "            list_adj = torch.tensor(batch[1]).to(device).float()\n",
    "            list_shift = torch.tensor(batch[2]).to(device).float()\n",
    "            list_intensity = torch.tensor(batch[3]).to(device).float()\n",
    "            shift_total.append(list_shift.tolist())\n",
    "            intensity_total.append(list_intensity.tolist())\n",
    "            list_shift = list_shift.view(-1,1)\n",
    "            list_intensity = list_intensity.view(-1,1)\n",
    "            \n",
    "            list_pred_shift, list_pred_intensity = model(list_feature, list_adj)\n",
    "            pred_shift_total.append(list_pred_shift.view(-1).tolist())\n",
    "            pred_intensity_total.append(list_pred_intensity.view(-1).tolist())\n",
    "        \n",
    "        mae_shift = 0\n",
    "        mae_intensity = 0\n",
    "        for i in range(len(shift_total)):\n",
    "            mae_shift += mean_absolute_error(shift_total[i], pred_shift_total[i])/len(shift_total[i])\n",
    "            mae_intensity += mean_absolute_error(intensity_total[i], pred_intensity_total[i])/len(intensity_total[i])\n",
    "        mae_shift /= len(shift_total)\n",
    "        mae_intensity /= len(intensity_total)\n",
    "        \n",
    "    return mae_shift, mae_intensity, shift_total, pred_shift_total, intensity_total, pred_intensity_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dict_partition, device, bar, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    model = GCNNet(args)\n",
    "    model.to(device)\n",
    "    \n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, 'Undefined Optimizer Type'\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    \n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "\n",
    "    data_train = DataLoader(dict_partition['train'], \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=args.shuffle)\n",
    "\n",
    "    data_val = DataLoader(dict_partition['val'],\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=args.shuffle)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        model, train_loss = train(model, device, optimizer, criterion, data_train, bar, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "        \n",
    "        model, val_loss = validate(model, device, criterion, data_val, bar, args)\n",
    "        list_val_loss.append(val_loss)\n",
    "        \n",
    "    data_test = DataLoader(dict_partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=args.shuffle)\n",
    "\n",
    "    mae_shift, mae_intensity, shift_total, pred_shift_total, intensity_total, pred_intensity_total = test(model, device, data_test, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_val_loss = list_val_loss\n",
    "    args.shift_total = shift_total\n",
    "    args.pred_shift_total = pred_shift_total\n",
    "    args.intensity_total = intensity_total\n",
    "    args.pred_intensity_total = pred_intensity_total\n",
    "    args.mae_shift = mae_shift\n",
    "    args.mae_intensity = mae_intensity\n",
    "    args.time_required = time_required\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 100\n",
    "args.lr = 0.0001\n",
    "args.l2_coef = 0\n",
    "args.optim = 'Adam'\n",
    "args.epoch = 30\n",
    "args.n_block = 2\n",
    "args.n_layer = 2\n",
    "args.n_atom = args.max_atoms\n",
    "args.in_dim = args.num_feature\n",
    "args.hidden_dim = 64\n",
    "args.readout_dim = 256\n",
    "args.shift_pred_dim1 = 256\n",
    "args.shift_pred_dim2 = 128\n",
    "args.intensity_pred_dim1 = 256\n",
    "args.intensity_pred_dim2 = 128\n",
    "args.out_dim = args.max_peaks\n",
    "args.bn = True\n",
    "args.sc = 'no'\n",
    "args.atn = False\n",
    "args.num_head = 16\n",
    "args.dropout = 0\n",
    "args.step_size = 10\n",
    "args.gamma = 0.1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931350e8c2964d9b8b221bc7088a36d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 250\n",
      "150 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-809f607893a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"result\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdict_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-04bee41aef1d>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(dict_partition, device, bar, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mlist_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-a2d8217c07e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, criterion, data_train, bar, args)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# [150, 100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlist_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlist_intensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlist_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_shift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "dict_result = dict()\n",
    "n_iter = args.epoch*(len(dict_partition['train'])+len(dict_partition['val']))\n",
    "bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n",
    "\n",
    "args.exp_name = \"result\"\n",
    "result = vars(experiment(dict_partition, device, bar, args))\n",
    "dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "bar.close()\n",
    "\n",
    "df_result = pd.DataFrame(dict_result).transpose()\n",
    "df_result.to_json('result.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
