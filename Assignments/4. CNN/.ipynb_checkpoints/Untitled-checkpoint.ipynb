{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f875e8f8330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "    \n",
    "    smi_list = list()\n",
    "    logP_list = list()\n",
    "    \n",
    "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
    "        smi = contents[i].strip()\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "        \n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "    \n",
    "    return smi_list, logP_list\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles)<max_length:\n",
    "            smiles +=\" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
    "        for i, elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "        \n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total = []\n",
    "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting Data'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "class OneHotLogPDataSet(Dataset):\n",
    "    def __init__(self, list_one_hot, list_logP):\n",
    "        self.list_one_hot = list_one_hot\n",
    "        self.list_logP = list_logP\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_one_hot)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.list_one_hot[index], self.list_logP[index]\n",
    "    \n",
    "def partition(list_one_hot, list_logP, args):\n",
    "    num_total = list_one_hot.shape[0]\n",
    "    num_train = int(num_total*(1-args.test_size-args.val_size))\n",
    "    num_val = int(num_total*args.val_size)\n",
    "    num_test = int(num_total*args.test_size)\n",
    "    \n",
    "    one_hot_train = list_one_hot[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    one_hot_val = list_one_hot[num_train:num_train+num_val]\n",
    "    logP_val = list_logP[num_train:num_train+num_val]\n",
    "    one_hot_test = list_one_hot[num_total-num_test:]\n",
    "    logP_test = list_logP[num_total-num_test:]\n",
    "    \n",
    "    train_set = OneHotLogPDataSet(one_hot_train, logP_train)\n",
    "    val_set = OneHotLogPDataSet(one_hot_val, logP_val)\n",
    "    test_set = OneHotLogPDataSet(one_hot_test, logP_test)\n",
    "    \n",
    "    partition = {\n",
    "        'train' : train_set,\n",
    "        'val' : val_set,\n",
    "        'test' : test_set\n",
    "    }\n",
    "    \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927314a844da48a087e832caf32cc4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading Data', max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798b74b261b34b13b121cc4f289b725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Converting Data', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles', 50000)\n",
    "list_one_hot = smiles_to_onehot(list_smi)\n",
    "dict_partition = partition(list_one_hot, list_logP, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, planes):\n",
    "        super(SkipConnectionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, \n",
    "                               planes, \n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        #torch.nn.init.xavier_uniform(self.conv1.weight)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes,\n",
    "                               planes,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    '''(32,120)->(15, 59)'''\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.branch_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        \n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3db1_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = self.branch_pool(x)\n",
    "        \n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 100\n",
    "args.lr = 0.001\n",
    "args.l2_coef = 0.001\n",
    "args.optim = optim.Adam\n",
    "args.criterion = nn.MSELoss()\n",
    "args.epoch = 10\n",
    "args.device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-16-45bc14d39670>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-45bc14d39670>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    args.device)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(SkipConnectionBlock(1, 64),\n",
    "                      Flatten(),\n",
    "                      nn.Linear(245760, 1))\n",
    "model.to(args.device)\n",
    "\n",
    "list_train_loss = list()\n",
    "list_val_loss = list()\n",
    "acc = 0\n",
    "\n",
    "optimizer = args.optim(model.parameters(),\n",
    "                       lr=args.lr,\n",
    "                       weight_decay=args.l2_coef)\n",
    "\n",
    "data_train = DataLoader(args.dict_partition['train'], \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle)\n",
    "print(\"Loaded data for training\")\n",
    "\n",
    "data_val = DataLoader(args.dict_partition['val'],\n",
    "                      batch_size=args.batch_size,\n",
    "                      shuffle=args.shuffle)\n",
    "print(\"Loaded data for validation\")\n",
    "\n",
    "increment = args.epoch//40\n",
    "point = args.epoch//100\n",
    "for epoch in range(args.epoch):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(data_train):\n",
    "        one_hots = torch.tensor(np.expand_dims(batch[0], axis=1),\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        logPs = torch.tensor(batch[1],\n",
    "                             dtype=torch.float,\n",
    "                             args.device)\n",
    "        logPs = logPs.view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_logPs = model(one_hots)\n",
    "        pred_logPs.require_grad = False\n",
    "        train_loss = args.criterion(pred_logPs, logPs)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \"\\tbatch: \", i, \"\\tTraining\")\n",
    "        \n",
    "    list_train_loss.append(epoch_train_loss/len(data_train))\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumertae(data_val):\n",
    "            one_hots = torch.tensor(np.expand_dims(batch[0], axis=1),\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "            logPs = torch.tensor(batch[1],\n",
    "                                 dtype=torch.float,\n",
    "                                 args.device)\n",
    "            logPs = logPs.view(-1, 1)\n",
    "            \n",
    "            pred_logPs = model(one_hots)\n",
    "            val_loss = args.criterion(pred_logPs, logPs)\n",
    "            epoch_val_loss += val_loss.item()\n",
    "            \n",
    "            print(\"Epoch: \", epoch, \"\\tbatch: \", i, \"\\tValidating\")\n",
    "    \n",
    "    list_val_loss.append(epoch_val_loss/len(data_val))\n",
    "    \n",
    "    sys.stdout.write(\"\\r[\"+\"=\"*(i//increment)+\" \"*((args.epoch-i)//increment)+\"]\"+str(i/point)+\"%\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "\n",
    "data_test = DataLoader(args.dict_partition['test'],\n",
    "                      batch_size=args.batch_size,\n",
    "                      shuffle=args.shuffle)\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    list_logP = list()\n",
    "    list_pred_logP = list()\n",
    "    for i, batch in enumerate(data_test):\n",
    "        one_hots = torch.tensor(np.expand_dims(batch[0], axis=1),\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        logPs = torch.tensor(batch[1],\n",
    "                             dtype=torch.float,\n",
    "                             device=args.device)\n",
    "        logPs = logPs.view(-1, 1)\n",
    "        \n",
    "        pred_logPs = model(one_hots)\n",
    "        \n",
    "        list_logP += torch.squeeze(toxs).tolist()\n",
    "        list_pred_logP += pred_logPs.tolist()\n",
    "        \n",
    "    acc = accuracy_score(list_logP, list_pred_logP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = DataLoader(args.dict_partition['train'], \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle)\n",
    "one_hots = list()\n",
    "logPs = list()\n",
    "for i, batch in enumerate(data_train):\n",
    "    if i==0:\n",
    "        one_hots = torch.tensor(np.expand_dims(batch[0], axis=1),\n",
    "                                dtype=torch.float,\n",
    "                                device=args.device)\n",
    "        logPs = torch.tensor(batch[1],\n",
    "                             dtype=torch.float,\n",
    "                             device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 32, 120]), torch.Size([100]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hots.shape, logPs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
